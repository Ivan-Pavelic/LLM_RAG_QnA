{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "65a871b2",
      "metadata": {},
      "source": [
        "# LLM vs RAG: Q&A and Summarization Experiments\n",
        "\n",
        "This notebook implements a reproducible comparison of:\n",
        "\n",
        "- **LLM baseline** (no retrieval)\n",
        "- **RAG-BM25** (sparse retrieval)\n",
        "- **RAG-Dense** (SentenceTransformers retrieval)\n",
        "\n",
        "on two tasks:\n",
        "\n",
        "- **Question Answering (Q&A)** using a small **SQuAD** subset\n",
        "- **Document summarization** using a small **CNN/DailyMail** subset\n",
        "\n",
        "## Research questions (explicit)\n",
        "1. Does RAG improve answer correctness compared to a standalone LLM?\n",
        "2. Does RAG reduce hallucinations in Q&A and summarization tasks?\n",
        "3. How do BM25 and dense retrieval differ in performance?\n",
        "4. Are automatic metrics aligned with manual evaluation?\n",
        "\n",
        "> For coursework scale we use small dataset subsets with fixed seeds. Increase sizes only after the pipeline is working.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05bfd977",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup & imports\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from llm_rag_qna.utils import ensure_dir, read_jsonl, set_seed\n",
        "from llm_rag_qna.chunking import chunk_corpus, simple_word_chunks\n",
        "from llm_rag_qna.retrievers import BM25Retriever, DenseRetriever\n",
        "from llm_rag_qna.generator import HFText2TextGenerator, GenerationConfig\n",
        "from llm_rag_qna.pipeline import (\n",
        "    RAGConfig,\n",
        "    answer_qa_llm,\n",
        "    answer_qa_rag,\n",
        "    summarize_llm,\n",
        "    summarize_rag_over_article,\n",
        ")\n",
        "from llm_rag_qna.evaluation import (\n",
        "    qa_accuracy,\n",
        "    compute_bleu,\n",
        "    compute_rouge,\n",
        "    compute_bertscore,\n",
        "    manual_label_to_numeric,\n",
        "    correlation_manual_vs_metric,\n",
        ")\n",
        "from llm_rag_qna import viz\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 120)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0648b23e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment configuration (edit these first)\n",
        "\n",
        "SEED = 13\n",
        "set_seed(SEED)\n",
        "\n",
        "# Dataset paths produced by: `python -m scripts.download_data`\n",
        "DATA_DIR = Path(\"data\") / \"processed\"\n",
        "QA_QUESTIONS_PATH = DATA_DIR / \"qa_questions.jsonl\"\n",
        "QA_CORPUS_PATH = DATA_DIR / \"qa_corpus.jsonl\"\n",
        "SUM_ARTICLES_PATH = DATA_DIR / \"sum_articles.jsonl\"\n",
        "\n",
        "# Chunking strategy\n",
        "# - word-based chunking is easy to describe and reproduce\n",
        "CHUNK_WORDS = 200\n",
        "OVERLAP_WORDS = 50\n",
        "\n",
        "# Retrieval\n",
        "TOP_K = 5\n",
        "DENSE_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Generator (same for baseline and RAG)\n",
        "GEN_MODEL = \"google/flan-t5-small\"  # switch to flan-t5-base for better quality\n",
        "GEN_CFG = GenerationConfig(max_new_tokens=128, temperature=0.0, top_p=1.0)\n",
        "\n",
        "# Metrics (BERTScore model can be heavy; choose a smaller one if needed)\n",
        "BERTSCORE_MODEL = \"distilroberta-base\"\n",
        "\n",
        "# Output files for tables / annotation\n",
        "OUT_DIR = ensure_dir(\"outputs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc93e7c3",
      "metadata": {},
      "source": [
        "## Dataset preparation\n",
        "\n",
        "If you have not downloaded the datasets yet, run in a terminal:\n",
        "\n",
        "```bash\n",
        "python -m scripts.download_data\n",
        "```\n",
        "\n",
        "This creates small, fixed-size subsets:\n",
        "- `qa_questions.jsonl` (≈80 Q&A pairs)\n",
        "- `qa_corpus.jsonl` (matching contexts to retrieve from)\n",
        "- `sum_articles.jsonl` (≈25 articles + reference highlights)\n",
        "\n",
        "Why these datasets:\n",
        "- **SQuAD** provides questions with *textual evidence* (context) which makes the \"answerable from retrieved documents\" constraint clear and testable.\n",
        "- **CNN/DailyMail** is a standard summarization benchmark; we use a small subset to keep runtime manageable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f25504f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "\n",
        "assert QA_QUESTIONS_PATH.exists(), f\"Missing: {QA_QUESTIONS_PATH}. Run `python -m scripts.download_data`.\"\n",
        "assert QA_CORPUS_PATH.exists(), f\"Missing: {QA_CORPUS_PATH}. Run `python -m scripts.download_data`.\"\n",
        "assert SUM_ARTICLES_PATH.exists(), f\"Missing: {SUM_ARTICLES_PATH}. Run `python -m scripts.download_data`.\"\n",
        "\n",
        "qa_questions = read_jsonl(QA_QUESTIONS_PATH)\n",
        "qa_corpus = read_jsonl(QA_CORPUS_PATH)\n",
        "sum_articles = read_jsonl(SUM_ARTICLES_PATH)\n",
        "\n",
        "print(\"QA questions:\", len(qa_questions))\n",
        "print(\"QA corpus docs:\", len(qa_corpus))\n",
        "print(\"Summarization articles:\", len(sum_articles))\n",
        "\n",
        "pd.DataFrame(qa_questions).head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ffebe05",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build QA retrieval index (chunk the QA corpus)\n",
        "\n",
        "qa_chunks = chunk_corpus(\n",
        "    qa_corpus,\n",
        "    text_key=\"text\",\n",
        "    id_key=\"doc_id\",\n",
        "    chunk_words=CHUNK_WORDS,\n",
        "    overlap_words=OVERLAP_WORDS,\n",
        ")\n",
        "print(\"QA chunks:\", len(qa_chunks))\n",
        "\n",
        "bm25_retriever = BM25Retriever(qa_chunks)\n",
        "dense_retriever = DenseRetriever(qa_chunks, model_name=DENSE_MODEL)\n",
        "\n",
        "rag_cfg = RAGConfig(top_k=TOP_K)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73f6d611",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize generator (same generator for baseline and RAG)\n",
        "\n",
        "generator = HFText2TextGenerator(model_name=GEN_MODEL)\n",
        "print(\"Generator device:\", generator.device)\n",
        "print(\"Generator model:\", generator.model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43bd350b",
      "metadata": {},
      "source": [
        "## Task 1: Question Answering\n",
        "\n",
        "We compare three systems:\n",
        "- **LLM**: prompt contains only the question\n",
        "- **RAG-BM25**: retrieve top-k chunks via BM25, then inject as context\n",
        "- **RAG-Dense**: retrieve top-k chunks via SentenceTransformers, then inject as context\n",
        "\n",
        "Key methodological controls:\n",
        "- same generator model\n",
        "- same decoding settings\n",
        "- same *instruction* prompt, except context injection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d007317",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run QA inference\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "qa_rows = []\n",
        "\n",
        "for ex in tqdm(qa_questions, desc=\"QA\"):\n",
        "    qid = ex[\"id\"]\n",
        "    question = ex[\"question\"]\n",
        "    ref = ex[\"answer\"]\n",
        "\n",
        "    pred_llm = answer_qa_llm(question, generator=generator, gen_cfg=GEN_CFG)\n",
        "    out_bm25 = answer_qa_rag(\n",
        "        question,\n",
        "        retriever=bm25_retriever,\n",
        "        rag_cfg=rag_cfg,\n",
        "        generator=generator,\n",
        "        gen_cfg=GEN_CFG,\n",
        "    )\n",
        "    out_dense = answer_qa_rag(\n",
        "        question,\n",
        "        retriever=dense_retriever,\n",
        "        rag_cfg=rag_cfg,\n",
        "        generator=generator,\n",
        "        gen_cfg=GEN_CFG,\n",
        "    )\n",
        "\n",
        "    qa_rows.append({\"task\": \"qa\", \"id\": qid, \"system\": \"llm\", \"prediction\": pred_llm, \"reference\": ref})\n",
        "    qa_rows.append({\"task\": \"qa\", \"id\": qid, \"system\": \"rag_bm25\", \"prediction\": out_bm25[\"answer\"], \"reference\": ref})\n",
        "    qa_rows.append({\"task\": \"qa\", \"id\": qid, \"system\": \"rag_dense\", \"prediction\": out_dense[\"answer\"], \"reference\": ref})\n",
        "\n",
        "qa_df = pd.DataFrame(qa_rows)\n",
        "qa_df.to_csv(OUT_DIR / \"predictions_qa.csv\", index=False)\n",
        "qa_df.head(6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d45b0112",
      "metadata": {},
      "outputs": [],
      "source": [
        "# QA automatic metrics\n",
        "\n",
        "qa_metrics = []\n",
        "\n",
        "for system, sdf in qa_df.groupby(\"system\"):\n",
        "    preds = sdf[\"prediction\"].tolist()\n",
        "    refs = sdf[\"reference\"].tolist()\n",
        "\n",
        "    m = {\n",
        "        \"system\": system,\n",
        "        \"qa_accuracy\": qa_accuracy(preds, refs),\n",
        "        # Additional text-overlap metrics (often imperfect for short answers):\n",
        "        \"bleu\": compute_bleu(preds, refs),\n",
        "        **compute_rouge(preds, refs),\n",
        "    }\n",
        "    # BERTScore is expensive; run once per system.\n",
        "    m.update(compute_bertscore(preds, refs, model_type=BERTSCORE_MODEL))\n",
        "    qa_metrics.append(m)\n",
        "\n",
        "qa_metrics_df = pd.DataFrame(qa_metrics)\n",
        "qa_metrics_df.to_csv(OUT_DIR / \"metrics_qa.csv\", index=False)\n",
        "qa_metrics_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88b132a4",
      "metadata": {},
      "source": [
        "## Task 2: Document summarization\n",
        "\n",
        "We compare the same three systems:\n",
        "- **LLM**: summarize the full article directly\n",
        "- **RAG-BM25**: chunk the article, retrieve the most relevant chunks (query = \"main points\"), then summarize only the retrieved chunks\n",
        "- **RAG-Dense**: same, but with dense retrieval\n",
        "\n",
        "Why this is a valid RAG summarization setting:\n",
        "- Many LLMs have context length limits; retrieval can act as a *content selector*.\n",
        "- The experiment is controlled because retrieval is performed within the same article (no external knowledge).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "750d2e3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run summarization inference\n",
        "\n",
        "sum_rows = []\n",
        "\n",
        "for ex in tqdm(sum_articles, desc=\"Summarization\"):\n",
        "    sid = ex[\"id\"]\n",
        "    article = ex[\"article\"]\n",
        "    ref = ex[\"highlights\"]\n",
        "\n",
        "    pred_llm = summarize_llm(article, generator=generator, gen_cfg=GEN_CFG)\n",
        "\n",
        "    # Chunk the single article for retrieval\n",
        "    art_chunks = simple_word_chunks(\n",
        "        doc_id=f\"sum_{sid}\",\n",
        "        text=article,\n",
        "        chunk_words=CHUNK_WORDS,\n",
        "        overlap_words=OVERLAP_WORDS,\n",
        "        meta={\"article_id\": sid},\n",
        "    )\n",
        "\n",
        "    out_bm25 = summarize_rag_over_article(\n",
        "        art_chunks,\n",
        "        retriever_type=\"bm25\",\n",
        "        rag_cfg=rag_cfg,\n",
        "        generator=generator,\n",
        "        gen_cfg=GEN_CFG,\n",
        "        dense_model_name=DENSE_MODEL,\n",
        "    )\n",
        "    out_dense = summarize_rag_over_article(\n",
        "        art_chunks,\n",
        "        retriever_type=\"dense\",\n",
        "        rag_cfg=rag_cfg,\n",
        "        generator=generator,\n",
        "        gen_cfg=GEN_CFG,\n",
        "        dense_model_name=DENSE_MODEL,\n",
        "    )\n",
        "\n",
        "    sum_rows.append({\"task\": \"sum\", \"id\": sid, \"system\": \"llm\", \"prediction\": pred_llm, \"reference\": ref})\n",
        "    sum_rows.append({\"task\": \"sum\", \"id\": sid, \"system\": \"rag_bm25\", \"prediction\": out_bm25[\"summary\"], \"reference\": ref})\n",
        "    sum_rows.append({\"task\": \"sum\", \"id\": sid, \"system\": \"rag_dense\", \"prediction\": out_dense[\"summary\"], \"reference\": ref})\n",
        "\n",
        "sum_df = pd.DataFrame(sum_rows)\n",
        "sum_df.to_csv(OUT_DIR / \"predictions_sum.csv\", index=False)\n",
        "sum_df.head(6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1e8807",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summarization automatic metrics\n",
        "\n",
        "sum_metrics = []\n",
        "\n",
        "for system, sdf in sum_df.groupby(\"system\"):\n",
        "    preds = sdf[\"prediction\"].tolist()\n",
        "    refs = sdf[\"reference\"].tolist()\n",
        "\n",
        "    m = {\n",
        "        \"system\": system,\n",
        "        \"bleu\": compute_bleu(preds, refs),\n",
        "        **compute_rouge(preds, refs),\n",
        "    }\n",
        "    m.update(compute_bertscore(preds, refs, model_type=BERTSCORE_MODEL))\n",
        "    sum_metrics.append(m)\n",
        "\n",
        "sum_metrics_df = pd.DataFrame(sum_metrics)\n",
        "sum_metrics_df.to_csv(OUT_DIR / \"metrics_sum.csv\", index=False)\n",
        "sum_metrics_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f214033",
      "metadata": {},
      "source": [
        "## Per-example scores (needed for boxplots and correlations)\n",
        "\n",
        "Aggregate averages are useful, but several required visualizations need *distributions* and *per-sample* values.\n",
        "\n",
        "We compute per-example **ROUGE-L F1** for both tasks (a common summarization metric that is also interpretable for QA short answers).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baf6adc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from llm_rag_qna.utils import normalize_text\n",
        "\n",
        "_scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def rougeL_f1(pred: str, ref: str) -> float:\n",
        "    return float(_scorer.score(ref, pred)[\"rougeL\"].fmeasure)\n",
        "\n",
        "all_df = pd.concat([qa_df, sum_df], ignore_index=True)\n",
        "\n",
        "all_df[\"rougeL_f1\"] = [rougeL_f1(p, r) for p, r in zip(all_df[\"prediction\"], all_df[\"reference\"]) ]\n",
        "all_df[\"exact_match_norm\"] = [\n",
        "    1.0 if normalize_text(p) == normalize_text(r) else 0.0\n",
        "    for p, r in zip(all_df[\"prediction\"], all_df[\"reference\"])\n",
        "]\n",
        "\n",
        "all_df.to_csv(OUT_DIR / \"predictions_all_scored.csv\", index=False)\n",
        "all_df.head(6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61515e82",
      "metadata": {},
      "source": [
        "## Mandatory visualizations\n",
        "\n",
        "You must include and interpret the following in the paper (Results section):\n",
        "- **Bar chart**: average metrics per model\n",
        "- **Box plot**: score distributions\n",
        "- **Scatter plot**: correlation between manual and automatic evaluation\n",
        "- **Radar (or line) chart**: multi-metric comparison\n",
        "- **Stacked bar chart**: hallucination breakdown (from manual labels)\n",
        "\n",
        "Below we generate figures and save them under `outputs/figures/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a994e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "FIG_DIR = ensure_dir(OUT_DIR / \"figures\")\n",
        "\n",
        "# 1) Bar charts: average metrics per model\n",
        "\n",
        "qa_bar = qa_metrics_df[[\"system\", \"qa_accuracy\", \"rougeL\", \"bertscore_F1\"]].copy()\n",
        "fig = viz.bar_avg_metrics(qa_bar, [\"qa_accuracy\", \"rougeL\", \"bertscore_F1\"], title=\"QA: average metrics by system\")\n",
        "fig.savefig(FIG_DIR / \"qa_bar_avg_metrics.png\", dpi=200)\n",
        "\n",
        "sum_bar = sum_metrics_df[[\"system\", \"rougeL\", \"bertscore_F1\", \"bleu\"]].copy()\n",
        "# Scale BLEU to 0..1 for comparability in the figure\n",
        "sum_bar[\"bleu\"] = sum_bar[\"bleu\"] / 100.0\n",
        "fig = viz.bar_avg_metrics(sum_bar, [\"rougeL\", \"bertscore_F1\", \"bleu\"], title=\"Summarization: average metrics by system\")\n",
        "fig.savefig(FIG_DIR / \"sum_bar_avg_metrics.png\", dpi=200)\n",
        "\n",
        "# 2) Box plots: score distributions (per-example ROUGE-L)\n",
        "long = all_df[[\"task\", \"system\", \"rougeL_f1\"]].rename(columns={\"rougeL_f1\": \"score\"})\n",
        "long[\"metric\"] = \"rougeL_f1\"\n",
        "\n",
        "for task in [\"qa\", \"sum\"]:\n",
        "    fig = viz.boxplot_distributions(long[long[\"task\"] == task][[\"system\", \"metric\", \"score\"]], title=f\"{task.upper()}: ROUGE-L F1 distribution\")\n",
        "    fig.savefig(FIG_DIR / f\"{task}_box_rougeL.png\", dpi=200)\n",
        "\n",
        "# 3) Radar chart: multi-metric comparison (summarization)\n",
        "rad = sum_metrics_df[[\"system\", \"rouge1\", \"rouge2\", \"rougeL\", \"bertscore_F1\", \"bleu\"]].copy()\n",
        "rad[\"bleu\"] = rad[\"bleu\"] / 100.0\n",
        "fig = viz.radar_multimetric(rad, [\"rouge1\", \"rouge2\", \"rougeL\", \"bertscore_F1\", \"bleu\"], title=\"Summarization: multi-metric comparison\")\n",
        "fig.savefig(FIG_DIR / \"sum_radar_multimetric.png\", dpi=200)\n",
        "\n",
        "print(\"Saved figures to:\", FIG_DIR.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0fb7686",
      "metadata": {},
      "source": [
        "## Manual evaluation (hallucination / correctness)\n",
        "\n",
        "Annotation scheme (3 labels):\n",
        "- **correct**\n",
        "- **partially_correct**\n",
        "- **incorrect_or_hallucinated**\n",
        "\n",
        "Why manual evaluation is required:\n",
        "- Automatic metrics (BLEU/ROUGE/BERTScore) can miss factual errors.\n",
        "- Hallucinations are *semantic* and *factual* phenomena; human judgment is the gold standard.\n",
        "\n",
        "### How to annotate\n",
        "1. This notebook creates a small CSV template under `outputs/manual_annotations_template.csv`.\n",
        "2. Open it in Excel/Google Sheets.\n",
        "3. For each row, fill `manual_label` using exactly one of the three labels above.\n",
        "4. Save as `outputs/manual_annotations_filled.csv`.\n",
        "\n",
        "Then rerun the next cell to generate:\n",
        "- **scatter plot** (manual vs ROUGE-L)\n",
        "- **stacked bar chart** (hallucination breakdown)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "037fbb32",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a manageable annotation template (same items across systems)\n",
        "\n",
        "MANUAL_QA_N = 30\n",
        "MANUAL_SUM_N = 10\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "qa_ids = qa_df[\"id\"].drop_duplicates().tolist()\n",
        "sum_ids = sum_df[\"id\"].drop_duplicates().tolist()\n",
        "\n",
        "qa_sample = rng.choice(qa_ids, size=min(MANUAL_QA_N, len(qa_ids)), replace=False)\n",
        "sum_sample = rng.choice(sum_ids, size=min(MANUAL_SUM_N, len(sum_ids)), replace=False)\n",
        "\n",
        "manual_df = all_df[\n",
        "    ((all_df[\"task\"] == \"qa\") & (all_df[\"id\"].isin(qa_sample)))\n",
        "    | ((all_df[\"task\"] == \"sum\") & (all_df[\"id\"].isin(sum_sample)))\n",
        "].copy()\n",
        "\n",
        "manual_df[\"manual_label\"] = \"\"  # fill this column\n",
        "manual_template_path = OUT_DIR / \"manual_annotations_template.csv\"\n",
        "manual_df.to_csv(manual_template_path, index=False)\n",
        "print(\"Wrote annotation template:\", manual_template_path.resolve())\n",
        "\n",
        "# If the filled file exists, load it and generate the remaining required figures\n",
        "filled_path = OUT_DIR / \"manual_annotations_filled.csv\"\n",
        "if filled_path.exists():\n",
        "    # Robust CSV reading for Excel/Sheets (delimiter sniff + BOM-safe)\n",
        "    filled = pd.read_csv(filled_path, sep=None, engine=\"python\")\n",
        "    filled.columns = [str(c).strip().lstrip(\"\\ufeff\") for c in filled.columns]\n",
        "\n",
        "    filled[\"manual_numeric\"] = manual_label_to_numeric(filled[\"manual_label\"].tolist())\n",
        "\n",
        "    # Scatter plot: manual vs automatic (ROUGE-L per-example)\n",
        "    fig = viz.scatter_manual_vs_metric(\n",
        "        filled.dropna(subset=[\"manual_numeric\", \"rougeL_f1\"]),\n",
        "        manual_numeric_col=\"manual_numeric\",\n",
        "        metric_col=\"rougeL_f1\",\n",
        "        title=\"Manual vs ROUGE-L F1 (all tasks)\",\n",
        "    )\n",
        "    fig.savefig(FIG_DIR / \"scatter_manual_vs_rougeL.png\", dpi=200)\n",
        "\n",
        "    # Correlation (Spearman) for reporting\n",
        "    rho = correlation_manual_vs_metric(filled, manual_col=\"manual_numeric\", metric_col=\"rougeL_f1\")\n",
        "    print(\"Spearman correlation (manual vs ROUGE-L F1):\", rho)\n",
        "\n",
        "    # Stacked bar: hallucination breakdown\n",
        "    fig = viz.stacked_hallucination_breakdown(\n",
        "        filled,\n",
        "        title=\"Manual label breakdown (proxy for hallucination/correctness)\",\n",
        "    )\n",
        "    fig.savefig(FIG_DIR / \"stacked_manual_breakdown.png\", dpi=200)\n",
        "\n",
        "    print(\"Saved manual-eval figures to:\", FIG_DIR.resolve())\n",
        "else:\n",
        "    print(\"Fill the template and save as:\", filled_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
